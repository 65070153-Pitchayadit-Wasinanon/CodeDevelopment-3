# Compiler <br>
Compiler คือโปรแกรมคอมพิวเตอร์ที่แปลงภาษาโปรแกรมระดับสูง (high-level programming language) ไปเป็นภาษาเครื่อง (machine language) ภาษาเครื่องเป็นภาษาที่ CPU ของคอมพิวเตอร์เข้าใจและใช้งานได้โดยตรง Compiler ทำหน้าที่แปลโค้ดที่เขียนโดยมนุษย์ให้เป็นชุดคำสั่งที่ CPU เข้าใจได้
## ขั้นตอนการทำงานของ Compiler
![How does the compiler work](https://media.geeksforgeeks.org/wp-content/uploads/20200524115722/Capture3311.png)<br>
[Lexical Analysis](#Lexical-Analysis)<br>
[Syntax Analysis](#Syntax-Analysis)<br>
![](https://media.geeksforgeeks.org/wp-content/uploads/compilerDesign.jpg)<br>
## Lexical Analysis (การวิเคราะห์ตัวอักษร)
<a name="Lexical-Analysis"></a>
เป็นขั้นตอนแรกในการดำเนินการคอมไพล์โปรแกรมที่เป็นภาษามนุษย์ไปเป็นภาษาเครื่อง ซึ่งมักจะเป็นขั้นตอนที่สำคัญในกระบวนการคอมไพล์โดยทั่วไป เป้าหมายหลักของ lexical analysis คือการแยกแยะและการจับคู่ส่วนประกอบของโค้ดโปรแกรมให้อยู่ในรูปแบบที่สามารถทำการวิเคราะห์และแปลงไปเป็นโค้ดที่เป็นภาษาเครื่องได้ ซึ่งมันมุ่งเน้นไปที่การตรวจสอบความถูกต้องของโค้ดในระดับของตัวอักษรหรือ Token ซึ่งเป็นชุดของตัวอักษรที่มีความหมายรวมกันเป็นหน่วยๆ ซึ่งมักจะรวมถึงคำสั่ง เครื่องหมาย และค่าคงที่ เป็นต้น <br>
![Lexical Analysis](https://binaryterms.com/wp-content/uploads/2021/11/Lexical-Analysis-in-Compiler.jpg)<br>
### Lexical Analyser ทำงานยังไง
<a name="Lexical-Analyser"></a>
* Input preprocessing:  ขั้นตอนนี้จะทำความสะอาดข้อความอินพุตเพื่อเตรียมสำหรับการวิเคราะห์ทางไวยากรณ์ ซึ่งรวมถึงการลบส่วนประกอบที่ไม่จำเป็นออกไป เช่น คำอธิบาย ช่องว่าง ตัวอักษรพิเศษ เป็นต้น<br>
* Tokenization: ขั้นตอนนี้จะแบ่งข้อความอินพุตออกเป็นหน่วยข้อมูลเล็กๆ ที่เรียกว่า "โทเค็น" โดยมักจะใช้รูปแบบหรือการแสดงออกแบบปกติ (regular expressions) ในการจับคู่ตัวอักษรในข้อความอินพุตกับรูปแบบต่างๆ ของโทเค็น
* Token classification: ขั้นตอนนี้จะกำหนดประเภทของแต่ละโทเค็น ตัวอย่างเช่น ในภาษาโปรแกรม ตัวแบ่งประเภทโทเค็น (lexer) อาจจำแนกคีย์เวิร์ด ตัวระบุ ตัวดำเนินการ และสัญลักษณ์วรรคตอน เป็นประเภทโทเค็นที่แตกต่างกัน
* Token validation: ขั้นตอนนี้จะตรวจสอบว่าโทเค็นแต่ละโทเค็นถูกต้องตามกฎของภาษาที่ใช้เขียน ตัวอย่างเช่น ตัวแบ่งประเภทโทเค็นอาจตรวจสอบว่า ชื่อตัวแปรเป็นตัวระบุที่ถูกต้อง หรือ ตัวดำเนินการมีไวยากรณ์ที่ถูกต้อง
* Output generation: ขั้นตอนสุดท้าย ตัวแบ่งประเภทโทเค็นจะสร้างผลลัพธ์ของขั้นตอนการวิเคราะห์ทางไวยากรณ์เบื้องต้น ซึ่งโดยทั่วไปเป็นรายการของโทเค็น รายการโทเค็นนี้สามารถส่งต่อไปยังขั้นตอนถัดไปของการแปลรหัสหรือการตีความ
```
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n - 1)
```
Lexical Analysis จะแยกโค้ด Python นี้เป็นโทเค็นดังต่อไปนี้:<br>

['def', 'factorial', '(', 'n', ')', ':', 'if', 'n', '==', '0', ':', 'return', '1', 'else', ':', 'return', 'n', '*', 'factorial', '(', 'n', '-', '1', ')']

## Syntax Analysis (การวิเคราะห์ไวยากรณ์)
<a name="Syntax-Analysis"></a>
Syntax Analysis หรือ Parsing จะทำหน้าที่ ตรวจสอบ ว่า โครงสร้างของโค้ด นั้น ถูกต้องตามกฎไวยากรณ์ของภาษาโปรแกรม หรือไม่ โดยใช้ parse tree หรือ abstract syntax tree (AST) แสดงโครงสร้างของโค้ด
ตัวอย่าง
```
x = 10 + 5

```
Lexical Analysis: แยก "x", "=", "10", "+", "5" ออกจากกัน<br>
Syntax Analysis: วิเคราะห์ว่า "x" เป็นตัวแปร "=" เป็นตัวดำเนินการ "10" และ "5" เป็นตัวถูกดำเนินการ และเรียงลำดับถูกต้อง<br>
Parse Tree:<br>
```
    =
  /   \
 x     +
      / \
     10  5
```
<br>
AST:<br>
```
=
|
x
|
+
|
10
5
```
<br>
